# -*- coding: utf-8 -*-
"""Bert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N69HNVRBW_MnTBB_zNAJO59DIaUQlpP3
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/drive/My Drive/IR Assignment 1"

#!pip install datasets

#!pip install bert-embedding --no-deps

#!pip install gluonnlp

#!pip install mxnet-cu101

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer 
nltk.download('wordnet')
nltk.download('stopwords')
lemmatizer = WordNetLemmatizer()
import re

"""Removing all the useless symbols"""
REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;!@]')
"""Just compile the correct symbols and messages"""
BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')
"""Removing the stopwords, using the nltk dictionary"""
STOPWORDS = set(stopwords.words('english'))
"""
    string: a string
    return: preprocessed string
    Takes text as input and 
        1) Makes everything lower case.
        2) Deletes '[/(){}\[\]\|@,;!@]' and then '[^0-9a-z #+_]'.
        3) Removes StopWords.
        4) Lemmatizes every word.
    """
def preprocess(text):
    text = text.lower()
    text = REPLACE_BY_SPACE_RE.sub(" " , text)  # replace REPLACE_BY_SPACE_RE symbols by space in text
    text = BAD_SYMBOLS_RE.sub("", text)         # delete symbols which are in BAD_SYMBOLS_RE from text
    textlist = text.split()
    text = [wrd for wrd in textlist if wrd not in STOPWORDS]
    text = " ".join(text)
    
    #Lemmatize
    lem_words = []
    for word in text.split():
        lem_words.append(lemmatizer.lemmatize(word))
    return " ".join(lem_words)

#Actual Dataset
import numpy as np
from datasets import load_dataset
"""Load the aeslc dataset for our use"""
dataset = load_dataset("aeslc")

s = dataset['train'][:]['email_body']
len(s)

arr = []
for i in range(len(s)):
    arr.append([i for i in s[i].split('\n') if len(i)!=0 ])
assert 14436 == len(arr)
arr=None

"""
A function to make our corpus from the dataset , which we put into the array s earlier.<br>
We make our corpus in the following way :
<li> We first split each email into an array of lines.</li>
<li> Then, we preprocess each string using our preprocess function defined earlier</li>
<li> We make a temporary array of an array of strings, and then finally add the temporary array to our corpus </li>
Finally, our corpus is ready.
"""
def make_corpus(s):
  corpus = []
  for email in s:
      email = email.split("\n")
      #print(email)
      p=[]
      #print()
      for i in range(len(email)):
          if len(email[i]) > 0:
              prep_s = preprocess(email[i])
              if len(prep_s) > 0:
                  p.append(prep_s)
      corpus.append(p)
  return corpus
corpus = make_corpus(s)    
len(corpus)

import mxnet as mx
from bert_embedding import BertEmbedding

ctx = mx.gpu(0)
""" We create a method for bert embedding our corpus , later on"""
be = BertEmbedding(ctx=ctx)

import pickle

"""
Making our embedded set here.<br>
We create an embedded array here using the bert method , and then write the same to a pickl file.<br>
We chose pickl file to reduce the size and store is as a hashmap, allowing us to search faster. <br>
"""
def embed(corpus):
  embeddings = []
  for email in range(len(corpus)):
      printc=True
      for i in be(corpus[email]):
          if email%500==0 and printc:
              print("Iteration: ", email)
              printc=False
          #print(i[1])
          #print(sum(i[1]))
          #print(len(i[1]))
          try:
              embeddings.append(sum(i[1])/len(i[1]))
          except:
              print(i, email, corpus[email])
  with open('Bert_Email_Embeddings.pkl', 'wb') as fp:
    pickle.dump(embeddings, fp) 
embeddings = embed(corpus)

"""Load the contents from our pickl file"""
with open ('Bert_Email_Embeddings.pkl', 'rb') as fp:
    itemlist = pickle.load(fp)
len(itemlist)

len(corpus)

"""
We index our emails here. Hence, we need both our corpus and embeddings here <br>
We make a corpus of our embedded emails using this function <br>
At the same time , we also add an index of the emails with serial numbers, so that we can directly search it up later on with the match <br> 
"""
def index_emails(corpus,embeddings):
    l=0 #INDEX THROUGH EVERY SENTENCE
    email_emb=[] #VEC OF EMAIL
    eml = []
    for email in corpus:
        sarr = [] #SENTENCE VECS
        for _ in email:
            if len(preprocess(_)) > 0:
                eml.append(_)
                sarr.append(embeddings[l])
                l+=1
        email_emb.append(sum(sarr)/len(sarr))
    len(sarr), l
    indx_to_email = dict()
    l=0
    for indx_email in range(len(corpus)):
        start = l
        for j in range(len(corpus[indx_email])):
            indx_to_email[l]=(start,indx_email)
            l+=1
    len(indx_to_email), l

"""
Defining a function to highlight our query in the given email <br>
<li> Our first argument will be the index of the email in which we are searching of our query </li>
<li> Our next argument is by default none.If we find one , we print the find line, else the entire email </li>
"""
def find_email(i_email, highlight_line=None):
    for i in range(len(s[i_email])):
        if i==highlight_line:
            print('\33[34m' + s[i_email][i] + '\033[0m')
        else:
            print(s[i_email][i])

"""
A function for defining the cosine scores between 2 given vectors.
"""
def cosine_score(vec1, vec2):
    return np.dot(vec1, vec2)/(np.linalg.norm(vec1) * np.linalg.norm(vec2))

"""
A function to search our emails. <br>
<li> Our first argument is the string/query for which we will be searching through our mails</li>
<li> Our next argument is by default set to 3. This decides the number of maximum arguments that we recieve for our query.
<ol><h4> We process our query as follows : </h4>
<li> We first split our query into different sentences </li>
<li> Then , we preprocess it using our function , and also convert it to embedded format </li>
<li> Now , we compute the cosine similarity score of each sentence embedding with that of the original one in our corpus.</li>
<li> We check for the cosine scores in the descending order, and add the top_n (default = 3) into a separate array. </li>
<li> Now , we can print this index array as our result </li>
</ol>
"""
def search(query, top_n=3):    
    l = 0
    for q in query:
        c = [i for i in q.split('\n') if len(i)!=0]
        que = [preprocess(i) for i in q.split('\n') if len(i)!=0 ]
        emb = be(que)
        for i in range(len(emb)):
            print("\33[35mQuery {} of {} \033[0m".format(l+1, len(emb)))
            e = emb[i][1]
            embb = sum(e)/len(e)
            similarity = []
            for i in itemlist:
                similarity.append(cosine_score(embb, i))
            len(similarity)
            sim = np.argsort(similarity)[::-1][:top_n]
            print("Query:", "\33[31m" + c[l] + "\033[0m", "\n")
            l+=1
            for i in sim:
                print("\33[33m" + "Cosine Similarity: {:.3f} \t Email {} \t Line {} \033[0m".format(similarity[i], indx_to_email[i][1], i-indx_to_email[i][0]))
                print("\33[34m" + corpus[indx_to_email[i][1]][i-indx_to_email[i][0]] + '\033[0m',  "\n")

query = ["I will fill in the Legal description of the property one I have received it. \n Please let me know how you wish to handle this and I will proceed."]
search(query, top_n=4)

search(["Just thought you might want to put it on the list of fun  things to do in the future."])

search(["I have made some really good friends and I would like to stay in touch with all of you."])

