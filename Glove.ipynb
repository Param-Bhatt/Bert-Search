{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Glove",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Param-Bhatt/Search-engine/blob/main/Glove.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyuW4T-uQuuy",
        "outputId": "28d58f4a-de4d-4212-9339-ec51e0d004c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.6/dist-packages (1.1.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from datasets) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.18.5)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.10)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.2)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.6.20)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfl0s799UNLp",
        "outputId": "0adea80e-980d-4ce0-8f36-06054c56aef4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "#!pip install pdoc3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pdoc3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/68/ce1035e720f1c5b80372164c389907d26cb488d600b42d136c75f587a427/pdoc3-0.9.1.tar.gz (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 2.8MB/s \n",
            "\u001b[?25hCollecting mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/37/0e706200d22172eb8fa17d68a7ae22dec7631a0a92266634fb518a88a5b2/Mako-1.1.3-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: markdown>=3.0 in /usr/local/lib/python3.6/dist-packages (from pdoc3) (3.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from mako->pdoc3) (1.1.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=3.0->pdoc3) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=3.0->pdoc3) (3.2.0)\n",
            "Building wheels for collected packages: pdoc3\n",
            "  Building wheel for pdoc3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pdoc3: filename=pdoc3-0.9.1-cp36-none-any.whl size=115994 sha256=7d29dcdbc94f92c6151df866ff088cf5ef5108615233af5b2ab9e978fafbca4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/0d/9e/b10defcfc3a3b76999ea945f0d57047340456e966e2184a451\n",
            "Successfully built pdoc3\n",
            "Installing collected packages: mako, pdoc3\n",
            "Successfully installed mako-1.1.3 pdoc3-0.9.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7UbQo0SUnct",
        "outputId": "6de892db-3aa6-49b5-ee18-6a0dc74c74ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "!pdoc --html glove.py "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/glove.py:11: DeprecationWarning: invalid escape sequence \\[\n",
            "  REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;!@]')\n",
            "/usr/local/lib/python3.6/dist-packages/IPython/utils/module_paths.py:29: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
            "  import imp\n",
            "2020-10-22 13:42:07.437488: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "<unknown>:11: DeprecationWarning: invalid escape sequence \\[\n",
            "html/glove.html\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMn8xSl0ONDR",
        "outputId": "bb27d113-4514-447b-8fe7-6912df7ca543",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "#!wget http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-22 12:26:15--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1        6%[>                   ]  49.94M  27.6MB/s               ^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghzoCnWSQxSr",
        "outputId": "c827d6e4-e11c-4d38-d814-66b760d977f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "#!unzip glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace glove.6B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace glove.6B.200d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace glove.6B.300d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F42ybAjZhzHm"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import numpy as np \n",
        "from nltk.corpus import stopwords\n",
        "from datasets import load_dataset\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;!@]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "es4bUu9mOY0X"
      },
      "source": [
        "def init_glove_embeddings(dimentions=300):\n",
        "    \"\"\"\n",
        "    Input: Dimentions of the Embeddings to use\\n\n",
        "    Builds the glove embeddings in a dicyionary from the file\\n\n",
        "    Output: Dictionary of the pretrained dictionary\\n\n",
        "    \"\"\"\n",
        "    embeddings_dict = {}\n",
        "    with open(\"glove.6B.\"+str(dimentions)+\"d.txt\", 'r') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], \"float32\")\n",
        "            embeddings_dict[word] = vector\n",
        "    return embeddings_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYRTzsclVDjH"
      },
      "source": [
        "def init_dataset():\n",
        "    \"\"\"\n",
        "    Input: None\\n\n",
        "    Builds the dataset\\n\n",
        "    Output: dataset\\n\n",
        "    \"\"\"\n",
        "    dataset = load_dataset(\"aeslc\")\n",
        "    s = dataset['train'][:]['email_body']\n",
        "    #Emails as \n",
        "    arr = []\n",
        "    for i in range(len(s)):\n",
        "        arr.append([i for i in s[i].split('\\n') if len(i)!=0 ])\n",
        "    assert 14436 == len(arr)\n",
        "    return s, arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Plc2dxjS74Fd"
      },
      "source": [
        "def build_corpus(s):\n",
        "    \"\"\"\n",
        "    Input: An array of each email contiguously\\n\n",
        "    Breaks each email by new line and builds corpus of sentences\\n\n",
        "    Output: sentence corpus\\n\n",
        "    \"\"\"\n",
        "    corpus = []\n",
        "    for email in s:\n",
        "        email = email.split(\"\\n\")\n",
        "        #print(email)\n",
        "        p=[]\n",
        "        #print()\n",
        "        for i in range(len(email)):\n",
        "            if len(email[i]) > 0:\n",
        "                #prep_s = preprocess(email[i])\n",
        "                prep_s = email[i]\n",
        "                if len(prep_s) > 0:\n",
        "                    p.append(prep_s)\n",
        "        corpus.append(p)\n",
        "    return corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3E4ohDTV7YO",
        "outputId": "60de416a-f108-4b08-e7ad-06498930469a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "def preprocess(text):\n",
        "    \"\"\"\n",
        "    Input: a string\\n\n",
        "    Takes text as input and\\n\n",
        "        1) Makes everything lower case.\\n\n",
        "        2) Deletes '[/(){}\\[\\]\\|@,;!@]' and then '[^0-9a-z #+_]'.\\n\n",
        "        3) Removes StopWords.\\n\n",
        "        4) Lemmatizes every word.\\n\n",
        "    Output: preprocessed string\\n\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = REPLACE_BY_SPACE_RE.sub(\" \" , text)  # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
        "    text = BAD_SYMBOLS_RE.sub(\"\", text)         # delete symbols which are in BAD_SYMBOLS_RE from text\n",
        "    textlist = text.split()\n",
        "    text = [wrd for wrd in textlist if wrd not in STOPWORDS]\n",
        "    text = \" \".join(text)\n",
        "    \n",
        "    #Lemmatize\n",
        "    lem_words = []\n",
        "    for word in text.split():\n",
        "        lem_words.append(lemmatizer.lemmatize(word))\n",
        "    return \" \".join(lem_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYfZ-NdINLyr"
      },
      "source": [
        "# Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ji9yfzf4WId7"
      },
      "source": [
        "def glove_embed(sent):\n",
        "    \"\"\"\n",
        "    Input: Sentence as string\\n\n",
        "    Finds the glove embeddings of each word and adds it for each sentence\\n\n",
        "    Output: Embeddings and the length of words taken to form the embeddings\\n\n",
        "    \"\"\"\n",
        "    sent = sent.lower()\n",
        "    embeddings = [0 for _ in embeddings_dict[\"king\"]]\n",
        "    length = 0\n",
        "    for word in sent.split():\n",
        "        try:\n",
        "            #print(word)\n",
        "            embeddings+=embeddings_dict[word]\n",
        "            length+=1\n",
        "        except:\n",
        "            #print(\"ERROR\", word)\n",
        "            pass\n",
        "            \n",
        "    return embeddings, length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jxXZaWk5sFK"
      },
      "source": [
        "def build_index(corpus):\n",
        "    \"\"\"\n",
        "    Input: Corpus\\n\n",
        "    Builds the index of embeddings of each sentence in the corpus\\n\n",
        "    Output: Embddeings Index of each sentence in the corpus\\n\n",
        "    \"\"\"\n",
        "    embeddings = []\n",
        "    sentence_corpus = []\n",
        "    for email in range(len(corpus)):\n",
        "        printc=True\n",
        "        #print(\"EMAIL\", corpus[email])\n",
        "        for i in corpus[email]:\n",
        "            #print(i)\n",
        "            sentence_corpus.append(i)\n",
        "            j, length = glove_embed(i)\n",
        "            #print(length)\n",
        "            if length<= 0:\n",
        "                #print(\"Sent not found\",  i)\n",
        "                embeddings.append(None)\n",
        "                continue\n",
        "            if email%1000==0 and printc:\n",
        "                print(\"Iteration: \", email)\n",
        "                printc=False\n",
        "            try:\n",
        "                embeddings.append(j/length)\n",
        "            except:\n",
        "                print(i, email)\n",
        "    assert len(embeddings) == len(sentence_corpus)\n",
        "    return embeddings, sentence_corpus\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P-kz8XK9-qn"
      },
      "source": [
        "def cosine_score(vec1, vec2):\n",
        "    \"\"\"\n",
        "    Input: Two embeddings of same dimentions\\n\n",
        "    Finds the cosine score among them\\n\n",
        "    Output: Cosine Similarity\\n\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if vec1.all() == None or vec2.all() == None:\n",
        "            return 0\n",
        "    except: \n",
        "        return 0\n",
        "    return np.dot(vec1, vec2)/(np.linalg.norm(vec1) * np.linalg.norm(vec2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFjYQA-4ClG6"
      },
      "source": [
        "def search(query, sentence_corpus, top_n=2):\n",
        "    \"\"\"\n",
        "    Input: query to search, Sentence corpus to search in , top n results\\n\n",
        "    Searches with cosing similarity among the embeddings\\n\n",
        "    Output: most similar items to query\\n\n",
        "    \"\"\"\n",
        "    q_emb = []\n",
        "    j, length = glove_embed(query.lower())\n",
        "    if length<= 0:\n",
        "        print(\"Sent not found\",  i)\n",
        "        q_emb.append(None)\n",
        "    try:\n",
        "        q_emb.append(j/length)\n",
        "    except:\n",
        "        print(i, email)\n",
        "    similarity = []\n",
        "    for emb in range(len(embeddings)):\n",
        "        score = cosine_score(embeddings[emb], q_emb[0])\n",
        "        #print(emb, cosine_score(embeddings[emb], q_emb[0]))\n",
        "        similarity.append(cosine_score(embeddings[emb], q_emb[0]))\n",
        "    sim = np.argsort(similarity)[::-1][:top_n]\n",
        "    print(\"Query:\", query)\n",
        "    print(top_n)\n",
        "    for index in range(top_n):\n",
        "        print(\"Cosine Score: {:.3f}, {}\".format(similarity[sim[index]], sentence_corpus[sim[index]]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3P82hTmZ4H2",
        "outputId": "c6314563-6f82-48fd-e01a-9fc6b790d956",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "print(\"Building Glove Dictionary...\")\n",
        "embeddings_dict = init_glove_embeddings()\n",
        "print(\"Built Glove Dictionary\")\n",
        "\n",
        "print(\"Initializing dataset...\")\n",
        "s, arr = init_dataset()\n",
        "print(\"Initialized Dataset\")\n",
        "\n",
        "print(\"Building corpus...\")\n",
        "corpus = build_corpus(s)\n",
        "print(\"Built corpus\")\n",
        "\n",
        "print(\"Building index...\")\n",
        "embeddings, sentence_corpus = build_index(corpus)\n",
        "print(\"Built index\")\n",
        "\n",
        "query = corpus[0][2]\n",
        "print(\"Searching Query:\", query)\n",
        "search(query, sentence_corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building Glove Dictionary...\n",
            "Built Glove Dictionary\n",
            "Initializing dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using custom data configuration default\n",
            "Reusing dataset aeslc (/root/.cache/huggingface/datasets/aeslc/default/1.0.0/0f335d7ea7f9ab9e325673a86c6aa0e2cd503c5dbf17d48fddcb02840a92fc96)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initialized Dataset\n",
            "Building corpus...\n",
            "Built corpus\n",
            "Building index...\n",
            "Iteration:  0\n",
            "Iteration:  1000\n",
            "Iteration:  2000\n",
            "Iteration:  3000\n",
            "Iteration:  4000\n",
            "Iteration:  5000\n",
            "Iteration:  6000\n",
            "Iteration:  7000\n",
            "Iteration:  8000\n",
            "Iteration:  9000\n",
            "Iteration:  10000\n",
            "Iteration:  11000\n",
            "Iteration:  12000\n",
            "Iteration:  13000\n",
            "Iteration:  14000\n",
            "Built index\n",
            "Searching Query: I will fill in the Legal description of the property one I have received it.\n",
            "Query: I will fill in the Legal description of the property one I have received it.\n",
            "2\n",
            "Cosine Score: 1.000, I will fill in the Legal description of the property one I have received it.\n",
            "Cosine Score: 0.959, I know that an issue has been raised about payment of incentives to employees on the North American payroll, but we have not received further clarification on the matter, so I would like to send you the \"Document\" which lists out who is included (by location and payroll) and what the definition of the Scheme is.\n",
            "CPU times: user 44.5 s, sys: 1.54 s, total: 46 s\n",
            "Wall time: 46.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKLb-T0bdHu4",
        "outputId": "05a71e89-ff91-475a-c40b-8ff39037eec8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "source": [
        "%%time\n",
        "search(query, sentence_corpus)\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Query: I will fill in the Legal description of the property one I have received it.\n",
            "2\n",
            "Cosine Score: 1.000, I will fill in the Legal description of the property one I have received it.\n",
            "Cosine Score: 0.959, I know that an issue has been raised about payment of incentives to employees on the North American payroll, but we have not received further clarification on the matter, so I would like to send you the \"Document\" which lists out who is included (by location and payroll) and what the definition of the Scheme is.\n",
            "\n",
            "CPU times: user 3.77 s, sys: 3.78 ms, total: 3.78 s\n",
            "Wall time: 3.79 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_0LAL4cmp8y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}