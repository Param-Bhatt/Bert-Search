# -*- coding: utf-8 -*-
"""Glove

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IiqHUYFz7lsfw0uOjG-N1EuND7rrwxQB
"""

#!pip install datasets

#!pip install pdoc3

#!pdoc --html glove.py

#!wget http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip

#!unzip glove.6B.zip

import re
import nltk
import numpy as np 
from nltk.corpus import stopwords
from datasets import load_dataset
from nltk.stem import WordNetLemmatizer 

nltk.download('wordnet')
nltk.download('stopwords')
lemmatizer = WordNetLemmatizer()
REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;!@]')
BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')
STOPWORDS = set(stopwords.words('english'))

def init_glove_embeddings(dimentions=300):
    """
    Input: Dimentions of the Embeddings to use\n
    Builds the glove embeddings in a dicyionary from the file\n
    Output: Dictionary of the pretrained dictionary\n
    """
    embeddings_dict = {}
    with open("glove.6B."+str(dimentions)+"d.txt", 'r') as f:
        for line in f:
            values = line.split()
            word = values[0]
            vector = np.asarray(values[1:], "float32")
            embeddings_dict[word] = vector
    return embeddings_dict

def init_dataset():
    """
    Input: None\n
    Builds the dataset\n
    Output: dataset\n
    """
    dataset = load_dataset("aeslc")
    s = dataset['train'][:]['email_body']
    #Emails as 
    arr = []
    for i in range(len(s)):
        arr.append([i for i in s[i].split('\n') if len(i)!=0 ])
    assert 14436 == len(arr)
    return s, arr

def build_corpus(s):
    """
    Input: An array of each email contiguously\n
    Breaks each email by new line and builds corpus of sentences\n
    Output: sentence corpus\n
    """
    corpus = []
    for email in s:
        email = email.split("\n")
        #print(email)
        p=[]
        #print()
        for i in range(len(email)):
            if len(email[i]) > 0:
                #prep_s = preprocess(email[i])
                prep_s = email[i]
                if len(prep_s) > 0:
                    p.append(prep_s)
        corpus.append(p)
    return corpus

def preprocess(text):
    """
    Input: a string\n
    Takes text as input and\n
        1) Makes everything lower case.\n
        2) Deletes '[/(){}\[\]\|@,;!@]' and then '[^0-9a-z #+_]'.\n
        3) Removes StopWords.\n
        4) Lemmatizes every word.\n
    Output: preprocessed string\n
    """
    text = text.lower()
    text = REPLACE_BY_SPACE_RE.sub(" " , text)  # replace REPLACE_BY_SPACE_RE symbols by space in text
    text = BAD_SYMBOLS_RE.sub("", text)         # delete symbols which are in BAD_SYMBOLS_RE from text
    textlist = text.split()
    text = [wrd for wrd in textlist if wrd not in STOPWORDS]
    text = " ".join(text)
    
    #Lemmatize
    lem_words = []
    for word in text.split():
        lem_words.append(lemmatizer.lemmatize(word))
    return " ".join(lem_words)

"""# Function"""

def glove_embed(sent):
    """
    Input: Sentence as string\n
    Finds the glove embeddings of each word and adds it for each sentence\n
    Output: Embeddings and the length of words taken to form the embeddings\n
    """
    sent = sent.lower()
    embeddings = [0 for _ in embeddings_dict["king"]]
    length = 0
    for word in sent.split():
        try:
            #print(word)
            embeddings+=embeddings_dict[word]
            length+=1
        except:
            #print("ERROR", word)
            pass
            
    return embeddings, length

def build_index(corpus):
    """
    Input: Corpus\n
    Builds the index of embeddings of each sentence in the corpus\n
    Output: Embddeings Index of each sentence in the corpus\n
    """
    embeddings = []
    sentence_corpus = []
    for email in range(len(corpus)):
        printc=True
        #print("EMAIL", corpus[email])
        for i in corpus[email]:
            #print(i)
            sentence_corpus.append(i)
            j, length = glove_embed(i)
            #print(length)
            if length<= 0:
                #print("Sent not found",  i)
                embeddings.append(None)
                continue
            if email%1000==0 and printc:
                print("Iteration: ", email)
                printc=False
            try:
                embeddings.append(j/length)
            except:
                print(i, email)
    assert len(embeddings) == len(sentence_corpus)
    return embeddings, sentence_corpus

def cosine_score(vec1, vec2):
    """
    Input: Two embeddings of same dimentions\n
    Finds the cosine score among them\n
    Output: Cosine Similarity\n
    """
    try:
        if vec1.all() == None or vec2.all() == None:
            return 0
    except: 
        return 0
    return np.dot(vec1, vec2)/(np.linalg.norm(vec1) * np.linalg.norm(vec2))

def search(query, sentence_corpus, top_n=2):
    """
    Input: query to search, Sentence corpus to search in , top n results\n
    Searches with cosing similarity among the embeddings\n
    Output: most similar items to query\n
    """
    q_emb = []
    j, length = glove_embed(query.lower())
    if length<= 0:
        print("Sent not found",  i)
        q_emb.append(None)
    try:
        q_emb.append(j/length)
    except:
        print(i, email)
    similarity = []
    for emb in range(len(embeddings)):
        score = cosine_score(embeddings[emb], q_emb[0])
        #print(emb, cosine_score(embeddings[emb], q_emb[0]))
        similarity.append(cosine_score(embeddings[emb], q_emb[0]))
    sim = np.argsort(similarity)[::-1][:top_n]
    print("Query:", query)
    print(top_n)
    for index in range(top_n):
        print("Cosine Score: {:.3f}, {}".format(similarity[sim[index]], sentence_corpus[sim[index]]))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# print("Building Glove Dictionary...")
# embeddings_dict = init_glove_embeddings()
# print("Built Glove Dictionary")
# 
# print("Initializing dataset...")
# s, arr = init_dataset()
# print("Initialized Dataset")
# 
# print("Building corpus...")
# corpus = build_corpus(s)
# print("Built corpus")
# 
# print("Building index...")
# embeddings, sentence_corpus = build_index(corpus)
# print("Built index")
# 
# query = corpus[0][2]
# print("Searching Query:", query)
# search(query, sentence_corpus)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# search(query, sentence_corpus)
# print()

