<!DOCTYPE html>
<html lang="en">

<head>
  <title>Analysis of our Search Engine</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
  <link href='https://fonts.googleapis.com/css?family=Literata' rel='stylesheet'>
  <style>
    body {
      font-family: 'Literata';
      font-size: 20px;
    }
  </style>
</head>

<body>
  <div class="container-fluid text-center">
    <div class="row content">
      <div class="col-sm-8 col-sm-offset-2 text-left">
        <h1>Analysis</h1>
        <hr>
        <ol>

        </ol>
        <h3>The tests and results of our Retrieval System: </h3>
        <ol>

          <li>
            <h3>
              Building the Indexes with and without preprocessing for both the embeddings, we found the following :
            </h3>
            <ul>
              <li>
                Preprocessing is actually faster in processing as it reduces the length of words to process because it
                discards words not in the embeddings, BERT does a better job as it tries to find the embeddings of
                subwords to find the meaning, rather than just discarding the sentence.
              </li><br>
              <li>
                BERT is highy parallel as the CPU user time is higher than the wall time, meaning that CPU worked more
                time that what elapsed in the real word, this is because the CPU is multithreaded.
              </li>
            </ul>
            <h3>The graphical analysis is shown below :</h3>
            <img src="resources/Index.jpeg" style="align-items: center;"><br><br>
            <img src="resources/Query.jpeg" style="align-items: center;">
            <p style="font-size: 20px;">
              <t>
                Here we can clearly see the difference of time taken by models:<br><br>
                <ol>
                  <li>
                    The major difference in index building time in BERT and Glove is because for Glove we are reading
                    the Embeddings from the dictionary, where as for BERT we have to find the embeddings which is
                    compute intensive. This is highly parallalized due to this implementation on GPU.
                  </li><br>
                  <li>
                    The difference in the query search time is very miniscule as here we are iterating through the
                    Index and finding for the cosing score. Since glove doesn't have embeddings for every word it
                    skips maky sentences in the dataset which makes it loose information, decreasing the quantum it
                    has to traverse.
                  </li>
                </ol>
              </t>
            </p>
          </li>
          <br>
          <li>
            <h3>Issues in Coding</h3><br>
            <p style="font-size: 20px;">
              <t>
                <ul>
                  <li>
                    We tried to work on embeddings but handeling large number of datastructures like Dictionary, list,
                    and combination of them, and the output of BertEmbeddings was challenging and interesting to
                    discover new datastructures to streamline the job.
                  </li><br>
                  <li>
                    The BERT Embeddings were found by a function which was compute intensive which took 5 hours in
                    google colab by CPU, we quickly realized that it is a parallizable task and the started to find
                    implementations of the task on GPU using module <i>Mxnet</i> which made the task faster by orders of
                    magnitude.
                  </li><br>
                  <li>
                    The Golve Embeddings were stored in a text file and we had to download them. The file had to be
                    processed to find a dictionary which had mappings from the words to the vectors.
                  </li>
                </ul>
              </t>
            </p><br>
          </li>

          <li>
            <h3>General Discussion </h3><br>
            <p style="font-size: 20px;">
              <t>
                <ul>
                  <li>
                    It was the right decision to work with embeddings even if we lose the positional meaning when we
                    average, as the embeddings have meanings of words which are too complex to be identified by TF-IDF.
                    We have examples (<i>weekend</i> is very similar to <i>before Friday</i>) to backup the theory that
                    embeddings understand the words and give better results.
                  </li><br>
                  <li>
                    Averaging the embeddings of the words diminishes the meaning of a specific word in the context, this
                    is significant in longer sentences where the model performs average.
                  </li><br>
                  <li>
                    This was a good excercise to see how 2 embedding models scale up to each other, even though BERT has
                    more than 2 times the number of dimentions than glove (786, 300) it performs a little better if not
                    as good as Glove. The gap will widen when we give positional information to the model.
                  </li><br>
                  <li>
                    Looking forward, We would work on attention mechanism to make the models put focus on important
                    phrases building up on the embeddings paradigm.
                  </li><br>
                </ul>
              </t>
            </p>
          </li>

          <li>
            <h2>Result:</h2><br>
            <p style="font-size: 20px;">
              <t>
                <ul>
                  <li>
                    We learnt about Embeddings and how to use them and what to do to imporve our retrieval system, in
                    both accuracy and time.
                  </li>
                  <li>
                    In addition to this, we learnt to interface cuda library and how to make our run time even shorter.
                  </li><br>
                </ul>
              </t>
            </p>
          </li>

          <li>
            <h3>References</h3>
            <p style="font-size: 20px;">
              <t>
                <ul>
                  <li><a href="https://geeksforgeeks.org" target="blank">Geeks for Geeks</a></li><br>
                  <li><a href="https://stackoverflow.com" target="blank"> Stackoverflow</a></li><br>
                  <li><a href="https://youtube.com" target="blank">Youtube tutorials</a></li><br>
                  <li><a href="https://github.com/google-research/bert" target="blank">Bert Embeddings</a></li><br>
                  <li><a href="https://nlp.stanford.edu/projects/glove/" target="blank">Glove Embeddings</a></li>
                </ul>
              </t>
            </p>
          </li>
        </ol>
      </div>
    </div>
  </div>