<!DOCTYPE html>
<html lang="en">

<head>
  <title>Tests</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
  <link href='https://fonts.googleapis.com/css?family=Literata' rel='stylesheet'>
  <style>
    body {
      font-family: 'Literata';
      font-size: 20px;
    }
  </style>
</head>

<body>
  <div class="container-fluid text-center">
    <div class="row content">
      <div class="col-sm-8 col-sm-offset-2 text-left">
        <h1>Analysis</h1>
        <hr>
        <ol>

        </ol>
        <h3>The tests and results of our Retrieval System: </h3>
        <ol>

          <li>
            <h3>
              Building the Indexes with and without preprocessing for both the embeddings, we found the following :
            </h3>
            <ul>
              <li>
                Preprocessing is actually faster in processing as it reduces the length of words to process because it
                discards words, not in the embeddings, BERT does a better job as it tries to find the embeddings of
                subwords to find the meaning, rather than just discarding the sentence.
              </li><br>
              <li>
                BERT is highly parallel as the CPU user time is higher than the wall time, meaning that the CPU worked
                more
                time that what elapsed in the real word, this is because the CPU is multithreaded.
              </li>
            </ul>
            <h3>The graphical analysis is shown below :</h3>
            <p style="align-items: center;">
              <img src="resources/Index.jpeg" style="width: 100%; height: auto;"><br><br>
              <img src="resources/Query.jpeg" style="width: 100%; height: auto;">
            </p>
            <p style="font-size: 20px;">
              <t>
                Here we can clearly see the difference of time taken by models:<br><br>
                <ol>
                  <li>
                    The major difference in index building time in BERT and Glove is because for Glove we are reading
                    the Embeddings from the dictionary, whereas for BERT we have to find the embeddings which are
                    computationally intensive. This is highly parallelized due to this implementation on GPU.
                  </li><br>
                  <li>
                    The difference in the query search time is very minuscule as here we are iterating through the
                    Index and finding for the cosine score. Since glove doesn't have embeddings for every word it
                    skips many sentences in the dataset which makes it lose information, decreasing the quantum it
                    has to traverse.
                  </li>
                </ol>
              </t>
            </p>
          </li>
          <br>
          <li>
            <h3>Issues in Coding</h3><br>
            <p style="font-size: 20px;">
              <t>
                <ul>
                  <li>
                    We tried to work on embeddings but handling a large number of data structures like Dictionary, list,
                    and combination of them, and the output of BertEmbeddings was challenging and interesting to
                    discover new data structures to streamline the job.
                  </li><br>
                  <li>
                    The BERT Embeddings were found by a computationally intensive function which took 5 hours in
                    google colab by CPU, we quickly realized that it is a parallelizable task and started to find
                    implementations of the task on GPU using module <i>Mxnet</i> which made the task faster by orders of
                    magnitude.
                  </li><br>
                  <li>
                    The Golve Embeddings were stored in a text file and we had to download them. The file had to be
                    processed to find a dictionary that had mappings from the words to the vectors.
                  </li>
                </ul>
              </t>
            </p><br>
          </li>

          <li>
            <h3>General Discussion </h3><br>
            <p style="font-size: 20px;">
              <t>
                <ul>
                  <li>
                    It was the right decision to work with embeddings even if we lose the positional meaning when we
                    average, as the embeddings have meanings of words that are too complex to be identified by TF-IDF.
                    We have examples (<i>weekend</i> is very similar to <i>before Friday</i>) to back-up the theory that
                    embeddings understand the words and give better results.
                  </li><br>
                  <li>
                    Averaging the embeddings of the words diminishes the meaning of a specific word in the context, this
                    is significant in longer sentences where the model performs average.
                  </li><br>
                  <li>
                    This was a good exercise to see how 2 embedding models scale up to each other, even though BERT has
                    more than 2 times the number of dimensions than glove (786, 300) it performs a little better if not
                    as good as Glove. The gap will widen when we give positional information to the model.
                  </li><br>
                  <li>
                    Looking forward, We would work on an attention mechanism to make the models put focus on important
                    phrases building upon the embeddings paradigm.
                  </li><br>
                </ul>
              </t>
            </p>
          </li>

          <li>
            <h2>Result:</h2><br>
            <p style="font-size: 20px;">
              <t>
                <ul>
                  <li>
                    We learned about Embeddings and how to use them and what to do to improve our retrieval system, in
                    both accuracy and time.
                  </li>
                  <li>
                    In addition to this, we learnt to interface cuda library and how to make our run time even shorter.
                  </li><br>
                </ul>
              </t>
            </p>
          </li>

          <li>
            <h3>References</h3>
            <p style="font-size: 20px;">
              <t>
                <ul>
                  <li><a href="https://geeksforgeeks.org" target="blank">Geeks for Geeks</a></li><br>
                  <li><a href="https://stackoverflow.com" target="blank"> Stackoverflow</a></li><br>
                  <li><a href="https://youtube.com" target="blank">Youtube tutorials</a></li><br>
                  <li><a href="https://github.com/google-research/bert" target="blank">Bert Embeddings</a></li><br>
                  <li><a href="https://nlp.stanford.edu/projects/glove/" target="blank">Glove Embeddings</a></li>
                </ul>
              </t>
            </p>
          </li>
        </ol>
      </div>
    </div>
  </div>